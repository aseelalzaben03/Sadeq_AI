
# -*- coding: utf-8 -*-
"""WEB search

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wEBdpDwnEDbZIusA0GX8Ks0LenDzJ88R
"""

import re
import time
import string
import requests
import gradio as gr
from bs4 import BeautifulSoup
from googlesearch import search
from sentence_transformers import SentenceTransformer, util
from sklearn.feature_extraction.text import CountVectorizer
from langdetect import detect

# ---------------- تنظيف نص ----------------
class TextCleaner:
    def __init__(self):
        self.punctuation_table = str.maketrans('', '', string.punctuation + '«»…“”–')

    def clean_text(self, text):
        text = text.lower()
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\d+', '', text)
        text = text.translate(self.punctuation_table)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

# ---------------- كلمات توقف ----------------
arabic_stopwords = ['في', 'من', 'على', 'و', 'عن', 'إلى', 'التي', 'الذي', 'أن', 'إن', 'كان', 'كما', 'لذلك', 'لكن',
                    'أو', 'ما', 'لا', 'لم', 'لن', 'قد', 'هذا', 'هذه', 'هو', 'هي', 'هم', 'ثم', 'كل', 'هناك', 'بعد']
english_stopwords = ['the', 'and', 'is', 'in', 'to', 'of', 'that', 'a', 'on', 'for', 'with',
                     'as', 'are', 'it', 'was', 'by', 'this', 'at', 'from', 'or', 'an', 'be']

# ---------------- كاشف اللغة ----------------
def detect_language(text):
    try:
        lang = detect(text)
        return 'ar' if lang == 'ar' else 'en'
    except:
        return 'en'

# ---------------- استخراج الكلمات المفتاحية ----------------
def extract_keywords_bert(text, lang, model, top_k=5):
    cleaner = TextCleaner()
    cleaned_text = cleaner.clean_text(text)
    stopwords = arabic_stopwords if lang == 'ar' else english_stopwords

    vectorizer = CountVectorizer(ngram_range=(1, 2), stop_words=stopwords).fit([cleaned_text])
    candidates = vectorizer.get_feature_names_out()

    doc_embedding = model.encode([cleaned_text], convert_to_tensor=True)
    candidate_embeddings = model.encode(candidates, convert_to_tensor=True)

    similarities = util.pytorch_cos_sim(doc_embedding, candidate_embeddings)[0]
    top_k_indices = similarities.topk(k=min(top_k, len(candidates))).indices

    top_keywords = [candidates[idx] for idx in top_k_indices]
    return " ".join(top_keywords)

# ---------------- بحث جوجل ----------------
class GoogleSearch:
    def __init__(self):
        self.headers = {"User-Agent": "Mozilla/5.0"}

    def get_page_text(self, url):
        try:
            response = requests.get(url, headers=self.headers, timeout=5)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                paragraphs = soup.find_all('p')
                text = ' '.join(p.get_text(strip=True) for p in paragraphs)
                return text[:1500]
        except:
            pass
        return ""

    def search(self, query, lang='ar'):
        results = {}
        try:
            urls = search(query, lang=lang, num_results=5)
            for url in urls:
                time.sleep(2)  # تأخير زمني بين الطلبات
                content = self.get_page_text(url)
                if content:
                    results[url] = content
        except Exception as e:
            print(f"Search error: {e}")
        return results

# ---------------- المحقق ----------------
class WebVerifier:
    def __init__(self):
        self.searcher = GoogleSearch()
        self.cleaner = TextCleaner()
        self.bert_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

    def search_web(self, text):
        lang = detect_language(text)
        keywords = extract_keywords_bert(text, lang, self.bert_model)
        return self.searcher.search(keywords, lang=lang)

    def get_most_similar_source(self, input_text, sources):
        input_text_clean = self.cleaner.clean_text(input_text)
        input_embedding = self.bert_model.encode(input_text_clean, convert_to_tensor=True)

        source_similarities = {}
        for url, content in sources.items():
            paragraphs = content.split('.')
            similarities = []
            for para in paragraphs:
                para = self.cleaner.clean_text(para)
                if len(para) > 30:
                    para_embedding = self.bert_model.encode(para, convert_to_tensor=True)
                    sim = util.pytorch_cos_sim(input_embedding, para_embedding).item()
                    similarities.append(sim)
            if similarities:
                source_similarities[url] = max(similarities)

        if not source_similarities:
            return None, 0.0

        most_similar_url = max(source_similarities, key=source_similarities.get)
        avg_similarity = sum(source_similarities.values()) / len(source_similarities)
        return most_similar_url, avg_similarity

    def verify_with_web(self, input_text):
        sources = self.search_web(input_text)
        most_similar_url, similarity = self.get_most_similar_source(input_text, sources)

        if similarity > 0.75:
            return f" موثوق\nأقرب مصدر: {most_similar_url}"
        elif similarity > 0.5:
            return f" مشكوك فيه\nأقرب مصدر: {most_similar_url}"
        else:
            return "غير موثوق"

    def verify_from_url(self, url):
        content = self.searcher.get_page_text(url)
        if not content or len(content) < 100:
            return " تعذر قراءة محتوى الرابط"
        return self.verify_with_web(content)

# ---------------- Gradio UI ----------------
verifier = WebVerifier()

def verify_news(input_text, input_url):
    if input_url.strip():
        return verifier.verify_from_url(input_url)
    elif input_text.strip():
        return verifier.verify_with_web(input_text)
    else:
        return "يرجى إدخال نص أو رابط للتحقق."

with gr.Blocks(title="تحقق من الأخبار - عربي / إنجليزي") as demo:
    gr.Markdown("## تحقق من الأخبار الكاذبة بالعربية أو الإنجليزية")

    with gr.Row():
        news_text = gr.Textbox(label=" نص الخبر", placeholder="أدخل نص الخبر...")
        news_url = gr.Textbox(label=" رابط URL", placeholder="أو أدخل رابط المقال...")

    verify_btn = gr.Button(" تحقق")
    result_output = gr.Textbox(label="النتيجة")

    verify_btn.click(fn=verify_news, inputs=[news_text, news_url], outputs=result_output)

demo.launch()

